---
title: "Unit 1 Section A"
---

# Computer Overview

## I. Introduction

A computer is an electronic device that can receive a set of
instructions, or program, and then carry out this program by
performing calculations on numerical data or by **manipulating**
other forms of information.

The modern world of high technology could not have come about except
for the development of the computer. Different types and sizes of
computers find uses throughout society in the storage and handling
data, from secret governmental files to banking transactions to
private household accounts. Computers have opened up a new era in
manufacturing through the techniques of automation, and they have
enhanced modern communication systems. They are essential tools in
almost every field of research and applied technology, from
constructing models of the universe to producing tomorrow's weather
reports, and their use has in itself opened up new areas of
**conjecture**. Database services and computer networks make available
a great variety of information sources. The same advanced techniques
also make possible **invasions** of personal and business **privacy**.
Computer crime has become one of the many risks that are part of the
price of modern technology.

## II. History

The first adding machine, a **precursor** of the digital computer, was
**devised** in 1642 by the French scientist, mathematician, and
**philosopher** Blaise Pascal. This device employed a series of
ten-toothed wheels, each tooth representing a digit from 0 to 9. The
wheels were connected so that numbers could be added to each other by
advancing the wheels by a correct number of teeth. In the 1670s the
German **philosopher** and mathematician Gottfried Wilhelm Leibniz
improved on this machine by **devising** one that could also multiply.

The French inventor Joseph-Marie Jacquard, in designing an automatic
loom, used thin, **perforated** wooden boards to control the weaving
of complicated designs. During the 1880s the American **statistician**
Herman Hollerith **conceived** the idea of using perforated cards,
similar to Jacquard's boards, for processing data. Employing a system
that passed punched cards over electrical contacts, he was able to
compile statistical information for the 1890 United States **census**.

### 1. The Analytical Engine

Also in the 19th century, the British mathematician and inventor
Charles Babbage worked out the principles of the modern digital
computer. He conceived a number of machines, such as the Difference
Engine, that were designed to handle complicated mathematical
problems. Many historians consider Babbage and his associate, the
mathematician Augusta Ada Byron, the true pioneers of the modern
digital computer. One of Babbage's designs, the Analytical Engine, had
many features of a modern computer. It had an input stream in the form
of a deck of punched cards, a "store" for saving data, a "mill" for
arithmetic operations, and a printer that made a permanent record.
Babbage failed to put this idea into practice, though it may well have
been technically possible at that date.

### 2. Early Computers

**Analogue** computers began to be build in the late 19th century.
Early models calculated by means of rotating **shafts** and gears.
Numerical **approximations** of equations too difficult to solve in
any other way were evaluated with such machines. Lord Kelvin built a
mechanical tide predictor that was a specialized analogue computer.
During World War I and II, mechanical and, later, electrical analogue
computing systems were used ad **torpedo** course predictors in
**submarines** and as **bombsight** controllers in aircraft. Another
system was designed to predict spring floods in the Mississippi River
basin.

### 3. Electronic Computers

During World War II, a team of scientists and mathematicians, working
at Bletchley Park, north of London, created one of the first
all-electronic digital computers: Colossus. By December 1943,
Colossus, which **incorporated** 1,500 **vacuum tubes**, was
operational. It was used by the team headed by Alan Turing, in the
largely successful attempt to crack German radio messages
**enciphered** in the Enigma code.

Independently of this, in the United States, a **prototype**
electronic machine had been built as early as 1939, by John Atanasoff
and Clifford Berry at Iowa State College. This prototype and later
research were completed quietly and later **overshadowed** by the
development of the Electronic Numerical **Integrator** And Computer
(ENIAC) in 1945. ENIAC was granted a **patent**, which was
**overturned** decades later, in 1973, when the machine was revealed
to have incorporated principles first used in the Atanasoff-Berry
Computer.

ENIAC contained 18,000 vacuum tubes and had a speed of several hundred
multiplications per minute, bur originally its program was wired into
the processor and had to be manually altered. Later machines were
build with program storage, based on the ideas of the
**Hungarian**-American mathematician John von Neumann. The
instructions, like the data, were stored within a "memory", freeing
the computer from the speed limitations of the paper-type reader
during execution and permitting problems to be solved without rewiring
the computer.

The use of the **transistor(( in computers in the late 1950s marked
the **advent** of smaller, faster, and more **versatile** **logical
elements** than were possible with vacuum-tube machines. Because
transistors use much less power and have a much longer life, this
development alone was responsible for the improved machines called
second-generation computers. Components became smaller, as did
inter-component spacings, and the system became much less expensive to
build.

### 4. Integrated Circuits

Late in the 1960s the **integrated circuit**, or IC, was introduced,
making it possible for many transistors to be **fabricated** on one
silicon **substrate**, with interconnecting wires plated in place. The
IC resulted in a further reduction in price, size and failure rate.
The microprocessor became a reality in the mid-1970s with the
introduction of the large-scale integrated (LSI) circuit and, later,
the very large-scale integrated (VLSI) circuit (microchip), with many
thousands of interconnected transistors **etched** into a single
silicon substrate.

To return, then, to the switching capabilities of a modern computer:
computers in the 1970s were generally able to handle eight switches at
a time. That is, they could deal with eight binary digits, or bits, of
data, at every cycle. A group of eight bits is called a byte, each
byte containing 256 possible patterns of ONs and OFFs (or 1s and 0s).
Each pattern is the **equivalent** of an instruction, a part of an
instruction, or a particular type of **datum**, such as a number or a
character or a **graphics** symbol. The pattern 11010010, for example,
might be binary data--in this case, the decimal number 210--or it
might be an instruction telling the computer to compare data stored in
its switches to data stored in a certain memory-chip location.

The development of processors that can handle 16, 32, and 64 bits of
data at a time has increased the speed of computers. The complete
collection of recognizable patterns--the total list of operations-- of
which a computer is capable is called its instruction set. Both
factors--the number of bits that can be handled at one time, and the
size of instruction sets--continue to increase with the ongoing
development of modern digital computers.

## III. Hardware

Modern digital computers are all conceptually similar, regardless of
size. Nevertheless, they can be divided into several categories on the
basis of cost and performance: the personal computer or
microcomputer, a relatively low-cost machine, usually of desktop size
(though "laptops" are small enough to fit in a briefcase, and
"palmtops" can fit into a pocket); the workstation, a microcomputer
with enhanced graphics and communications capabilities that make it
especially useful for office work; the minicomputer, generally too
expensive for personal use, with capabilities suited to a business,
school, or laboratory; and the mainframe computer, a large, expensive
machine with the capability of serving the needs of major business
enterprises, government departments, scientific research
establishments, or the like (the largest and fastest of these are
called supercomputers).

A digital computer is not a single machine: rather, it is a system
composed of five distinct elements: (1) a central processing unit; (2)
input devices; (3) memory storage devices; (4) output devices; and (5)
a communications network, called a bus, which links all the elements
of the system and connects the system to the external world.

## IV. Programming

A program is a sequence of instructions that tells the hardware of a
computer what operations to perform on data. Programs can be built
into the hardware itself, or they may exist independently in a form
known as software. In some specialized, or "dedicated", computers the
operating instructions are embedded in their **circuitry**; common
examples are the microcomputers found in calculators,
**wristwatches**, car engines, and microwave ovens. A general-purpose
computer, on the other hand, although it contains some **built-in**
programs (in ROM) or instructions (in the processor chip), depends on
external programs to perform useful tasks. Once a computer has been
programmed, it can do only as much or as little as the software
controlling it at any given moment enables it to do. Software in
widespread use includes a wide range of applications
programs--instructions to the computer on how to perform various
tasks.

## V. Future Developments

There is active research to make computers out of many promising new
types of technology, such as optical computers, DNA computers,
**neural** computers, and **quantum** computers. Most computers are
universal, and are able to calculate any computable function, and are
limited only bu their memory capacity and operating speed. However,
different designs of computers can give very different performance for
particular problems; for example, quantum computers can potentially
break some modern **encryption algorithms** (by quantum **factoring**)
very quickly.

A computer will solve problem in exactly the way it is programmed to,
without regard to efficiency, alternative solutions, possible
**shortcuts**, or possible errors in the code. Computers programs that
learn and adapt are part of the emerging field of artificial
intelligence and machine learning. Artificial intelligence-based
products generally fall into two major categories: rule-based systems
and pattern recognition systems, Rule-based systems attempt to
represent the rules used by human experts and tend to be expensive to
develop. Pattern-based systems use data about a problem to generate
conclusions. Examples of pattern-based systems include voice
recognition, font recognition, translation and the emerging field of
on-line marketing.
